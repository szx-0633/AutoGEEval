# AutoGEEval

AutoGEEval is a framework designed to test the ability of Large Language Models (LLMs) to generate Google Earth Engine (GEE) code. It supports multiodal outputs and automated evaluation using the GEE Python API.

## Workflow

Here's how to use AutoGEEval:

1.  **Setup API Keys and Project Configuration:**
    *   Apply for a GEE project and API keys from various providers.
    *   Fill in your GEE project details in `gee_py_test_v3.py`.
    *   Copy `llm_config.example.yaml` to `llm_config.yaml` and fill in your LLM API keys.

2.  **Construct the Test Dataset:**
    *   Run `construct_dataset.py` to call the LLM and build an initial test dataset.
    *   **Important:** The dataset generated by this script requires manual review and adjustment before it can be used for evaluation.
    *   The `dataset_complete` folder contains a manually reviewed dataset that is ready for evaluation.

3.  **Generate LLM Answers:**
    *   Configure the desired LLM settings.
    *   Execute `generate_llm_answer.py` to get the code completion results for the tasks in your dataset.

4.  **Evaluate GEE Code Generation:**
    *   Specify the model whose generated code you want to evaluate.
    *   Run `gee_py_test.py` to assess the GEE code generated by the LLM.

5.  **Process Results and Calculate Metrics:**
    *   Execute `process_test_reports.py` followed by `classify_errors.py`.
    *   These scripts will organize the results and calculate evaluation metrics such as pass@k and error classifications.

---

# AutoGEEval

AutoGEEval 是一个用于测试大语言模型 (LLM) 生成 Google Earth Engine (GEE) 代码能力的测试框架，支持GEE python API、多模态输出和自动化评测。

## 使用流程

以下是 AutoGEEval 的使用步骤：

1.  **配置 API 密钥和项目:**
    *   申请 GEE project 和各类 LLM 提供商的 API 密钥。
    *   将您的 GEE project 信息填入 `gee_py_test_v3.py` 文件中。
    *   将 `llm_config.example.yaml` 文件复制为 `llm_config.yaml`，并填入您的 LLM API 密钥。

2.  **构建测试数据集:**
    *   运行 `construct_dataset.py` 脚本，调用大语言模型来构建初始的测试数据集。
    *   **重要提示:** 此脚本生成的数据集需要经过人工审核和调整后才能用于最终测评。
    *   `dataset_complete` 文件夹中存放的是经过人工审核后、可直接用于测评的数据集。

3.  **生成大语言模型答案:**
    *   设置您想要运行测试的大语言模型。
    *   执行 `generate_llm_answer.py` 脚本，获取模型针对数据集中任务的补全结果。

4.  **评价 GEE 代码生成结果:**
    *   设置您想要评价其生成结果的模型。
    *   运行 `gee_py_test.py` 脚本，评测模型生成的 GEE 代码。

5.  **结果整理和指标计算:**
    *   依次执行 `process_test_reports.py` 和 `classify_errors.py` 脚本。
    *   这些脚本会整理测试报告并计算如 pass@k、错误分类等评价指标。